##
## Snakefile_0KnuttReads2Bins - The Snakefile combining the subfiles.
##
## Knutt.org/KnuttReads2Bins

# It mainly takes care of sample read globbing. It also contains most
# of the user facing rules.

import os
from itertools import product, repeat


# Check Snakemake Version
#snakemake.__version__

configfile: "config.yml"

reads = ["R1","R2"]

basedir_bench = config["output_dir"] + "/Benchmarking"
basedir_dbs = config["reference_dir"]
basedir_reporting = config["output_dir"] + "/Reports"
basedir_data = config["output_dir"] + "/Data"

lthread = config["threadcount_low_mem"]
mthread = config["threadcount_med_mem"]
hthread = config["threadcount_high_mem"]

paired_readfile_pattern = os.path.join(config["read_input_dir"],
                                       config["paired_read_files"])

# The globbing result for paired reads
# Is a iterable of named tuples
paired_read_glob = glob_wildcards(paired_readfile_pattern)

sample_names = list(set(paired_read_glob.sample))

# The samples with paired reads
# A dict, key: sample name
# value: dict (R1:read1file,R2:read2file)
paired_reads = {sample: {"R1":expand(paired_readfile_pattern,
                                     sample=sample, read="R1")[0],
                         "R2":expand(paired_readfile_pattern,
                                     sample=sample, read="R2")[0]} 
               for sample in sample_names}

# A sample and read pairing dict, convertible to a R data.frame
samples_names_reads = {"sample": [sample for sample in sample_names for _ in reads], "read":[read for _ in sample_names for read in reads]}

wildcard_constraints:
   read = "|".join(reads),
   sample = "|".join(sample_names)

include: "Snakefile_1PrepareReads"
include: "Snakefile_2ClassifyReads"
include: "Snakefile_3AnnotateReads"
include: "Snakefile_4Assemble"
include: "Snakefile_5Binning"

rule all:
   version: "1.0"
   input:
      expand(classfication_fastq(), sample=sample_names),
      expand(trimming_res + "R1_adptr_tr.fastq.gz", sample=sample_names),
      expand(merge_data + "/{sample}_qtr_{trimmed}.tsv",  sample=sample_names, trimmed=trimmed_val),
      expand(merge_data_file + "overview.tsv", trimmed=adpt_poss, sample=sample_names),
      expand(basedir_data_readclass + "/{sample}_readclassification_SSU.tsv", sample=sample_names),
      expand(basedir_data_readclass + "/{sample}_readclassification_kaiju.tsv", sample=sample_names),
      expand(basedir_data_readclass + "/{sample}_readclassification_sourmash_{sourtype}.tsv", sample=sample_names, sourtype=["lca", "gather"]),
      expand(basedir_data_readclass + "/sourmash_sample_comparison_{comptype}.tsv", comptype=samplecomptypes),
      expand(basedir_data_readclass + "/{sample}_sourmash_signature.tsv", sample=sample_names),
      expand(basedir_data_readanno + "/{sample}_readanno_{dbswithkrona}.tsv", dbswithkrona=integrateddbs + customdbs, sample=sample_names),
      expand(basedir_assembly + "/MEGAHIT/{sample}/final.contigs.fa", sample=sample_names),
      expand(basedir_data_assembly + "/{sample}_assembly_graph.tsv", sample=sample_names),
      expand(basedir_assembly + "/Graphs/{sample}_k{kmer}_graph.png", sample=sample_names, kmer=config["kmer_sizes_graph"]),
      expand(basedir_data_assembly + "/{sample}_cov_pileup_summary.tsv", sample=sample_names),
      expand(basedir_data_assembly + "/{sample}_metaquast.tsv", sample=sample_names),
      expand(basedir_data_assembly + "/{sample}_contigs_sourmash_lca.tsv", sample=sample_names),
      expand(basedir_data_assembly + "/{sample}_contigs_sourmash_signature.tsv", sample=sample_names),
      expand(basedir_data_assembly + "/{sample}_contigs_cat.tsv", sample=sample_names),
      expand(basedir_data_binning + "/{sample}_checkm.tsv", sample=sample_names),
      expand(basedir_data_binning + "/{sample}_checkm_profile.tsv",  sample=sample_names),
      expand(basedir_data_binning + "/{sample}_binmap.tsv", sample=sample_names),
      expand(basedir_data_binning + "/{sample}_bins_sourmash_classification.tsv", sample=sample_names),
      expand(basedir_data_binning + "/{sample}_bins_sourmash_signature.tsv", sample=sample_names),
      expand(basedir_data_binning + "/{sample}_bat.tsv", sample=sample_names),
      expand(basedir_data_binning + "/{sample}_bins_sourmash_search.tsv", sample=sample_names),
      expand(basedir_data_binning + "/sourmash_bin_comparison_{comptype}.tsv", comptype=samplecomptypes)
   message:
      "Ran everything, yay!"


rule allReport:
      version: "1.0"
      input:
         basedir_reporting + "/1raw-reads.html",
         basedir_reporting + "/2trimming.html",
         basedir_reporting + "/3read-merging.html",
         basedir_reporting + "/4read-ana-prep.html",
         basedir_reporting + "/5.1SSUreport.html",
         basedir_reporting + "/5.2kaijureport.html",
         basedir_reporting + "/5.3sourmashReport.html",
         basedir_reporting + "/SSU_krona.html",
         basedir_reporting + "/kaiju_krona.html",
         basedir_reporting + "/sourmash_krona.html",
         expand(basedir_reporting + "/6readanno_{dbswithkrona}.html", dbswithkrona=integrateddbs + customdbs),
         expand(basedir_reporting + "/readanno_{dbswithkrona}_krona.html", dbswithkrona=integrateddbs + customdbs),
         basedir_reporting + "/readanno_CAZyDB_funkrona.html",
         basedir_reporting + "/7assembly.html",
         basedir_reporting + "/CAT_krona.html",
         basedir_reporting + "/sourmash_contig_krona.html",
         basedir_reporting + "/8binning.html",
         basedir_reporting + "/BAT_krona.html"

rule refData:
   version: "1.0"
   input:
      rules.classifyReadsRefData.input,
      rules.readAnnoRefData.input,
      rules.catRefData.input,
      rules.assemblyRefData.input,
