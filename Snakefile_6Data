##
## Snakefile_6Data - Data extraction rules
##
## Knutt.org/Knutt2Reads2Bins

# Rules which depend on the other parts of the workflow and generate
# tsv files for every step. They are not needed to get from read to bins.


from itertools import product

basedir_data = config["output_dir"]+"/Data"

##
## Raw read prep
##
fastqlocations = {"trimmed":basedir_prep+"/Adapter_Trimming/{sample}/{sample}_{readtype}_adptr_tr.fastq.gz",
                  "merging_tr":basedir_prep+"/Merging_"+adpt_poss[0]+"/{sample}/{sample}_"+adpt_poss[0]+"_{readtype}.fastq.gz",
                  "merging_untr":basedir_prep+"/Merging_"+adpt_poss[1]+"/{sample}/{sample}_"+adpt_poss[1]+"_{readtype}.fastq.gz",
                  "qctrimmed_tr":basedir_prep+"/Merging_"+adpt_poss[0]+"/{sample}/{sample}_"+adpt_poss[0]+"_{readtype}_qtr.fastq.gz",
                  "qctrimmed_untr":basedir_prep+"/Merging_"+adpt_poss[1]+"/{sample}/{sample}_"+adpt_poss[1]+"_{readtype}_qtr.fastq.gz",
                  "readanno":classfication_fastq()
                  }
readtypes_fortype = {"raw":["R1","R2"],"trimmed":["R1","R2"],
                     "merging_tr":["merged","unmgd_R1","unmgd_R2"],
                     "merging_untr":["merged","unmgd_R1","unmgd_R2"],
                     "qctrimmed_tr":["merged","unmgd_R1"],
                     "qctrimmed_untr":["merged","unmgd_R1"],
                     "readanno":["merged_and_R1_qtr"]}

readtype_con = "|".join({rtype for types in readtypes_fortype.values() for rtype in types})
seqtype_con = "|".join(set(readtypes_fortype.keys())|{"raw"})

wildcard_constraints:
   seqtype = seqtype_con,
   readtype = readtype_con,
   seqtypebef = seqtype_con,
   seqtypeaf = seqtype_con

def fastqfile(w,seqtype):
   if(seqtype=="raw"):
      res = paired_reads[w["sample"]][w["readtype"]]
   else:
      res = expand(fastqlocations[seqtype],**w)[0]
   return res

seqdata_dir = basedir_data + "/Sequence_Data/"
seqdata_file_template = seqdata_dir + "{sample}_{readtype}_{seqtype}_"


seqdata_files = ["overview","plotdata"]

# Construct the files with the FASTQ information 
rule seqdata_file:
   input:
      reads=lambda w:fastqfile(w,w["seqtype"])
   output:
      overview=temp(seqdata_file_template+"overview.tsv"),
      toplot=temp(seqdata_file_template+"plotdata.tsv"),
   conda:
      "envs/R.yml"
   script:
      "scripts/DataExtraction/FASTQ_Data.R"  

# Combine these files
rule seqdata:
   input:
      files = lambda w: expand(seqdata_file_template + "{{seqdat}}.tsv", sample=sample_names, readtype=readtypes_fortype[w["seqtype"]], **w)
   params:
      colnames = ["sample","readdirection"],
      vals = lambda w: product(sample_names, readtypes_fortype[w["seqtype"]])
   output:
      out = seqdata_dir + "{seqtype}_{seqdat}.tsv"
   script:
      "scripts/DataExtraction/dataConcat.py"

# Compare two fastq files
rule compseqdata_file:
   input:
      before = lambda w:fastqfile(w,w["seqtypebef"]),
      after = lambda w:fastqfile(w,w["seqtypeaf"])
   output:
      basedir_data+"/Sequence_Data/{sample}_{readtype}_compare_{seqtypebef}_{seqtypeaf}.tsv"
   conda:
      "envs/R.yml"
   script:
      "scripts/DataExtraction/FASTQ_Comp_Data.R"  


def readtype_intersect(w):
   a = set(readtypes_fortype[w["seqtypebef"]])
   b = set(readtypes_fortype[w["seqtypeaf"]])
   return a&b 

# Combine the comparative files
rule compseqdata:
   input:
      files=lambda w: expand(rules.compseqdata_file.output,sample=sample_names, 
                       readtype=readtype_intersect(w),**w)
   params:
      colnames=["sample","readdirection"],
      vals=lambda w:product(sample_names,readtype_intersect(w))
   output:
      out=basedir_data+"/Sequence_Data/compare_{seqtypebef}_{seqtypeaf}.tsv"
   script:
      "scripts/DataExtraction/dataConcat.py"
   
# Construct adapter trimming data
rule adapterTrimmingData:
   input:
      files = expand(rules.cutadapt_paired_reads.log,sample=sample_names),
   params:
      colnames=["sample"],
      vals=sample_names
   output:
      out=basedir_data+"/ReadPrep/cutadapt_adptr_trimming.tsv"
   script:
      "scripts/DataExtraction/dataConcat.py"

# Get bbmerge merging data
# This rule needs to use the log file, because some info (adapter count, 
# ambigous count (A status missing, maybe bug?) isn't in the insert file)
rule merge_data_sample:
   input:
      log = rules.merge_paired_reads.log,
      adapter = rules.merge_paired_reads.output.adapters
   output:
      out=basedir_data+"/ReadPrep/{sample}_bbmerge_adptr_{trimmed}.tsv"
   conda:
      "envs/R.yml"
   script:
      "scripts/DataExtraction/bbmergeParser.R"

# Combine the data from the trimmed or untrimmed merge run for the samples
rule mergeData:
   input:
      files = expand(rules.merge_data_sample.output,sample=sample_names,trimmed="{trimmed}"),
   params:
      colnames=["sample"],
      vals=sample_names
   output:
      out=basedir_data+"/ReadPrep/bbmerge_overview_adptr_{trimmed}.tsv"
   script:
      "scripts/DataExtraction/dataConcat.py"

# Combine the insert detail files
rule mergeInsertData:
   input:
      files = expand(rules.merge_paired_reads.output.inserts,sample=sample_names,trimmed="{trimmed}"),
   params:
      colnames=["sample"],
      vals=sample_names
   output:
      out=basedir_data+"/ReadPrep/bbmerge_inserts_adptr_{trimmed}.tsv"
   script:
      "scripts/DataExtraction/dataConcat.py"

# Construct read analysis quality trimming data
rule qulitytrim_data:
   input:
      files = lambda w: expand(rules.qualtrim_merge_reads.log.cut,mergeres=["merged","unmgd_R1"],sample=sample_names,**w),
   params:
      colnames=["sample","readdirection"],
      vals=product(sample_names,["merged","unmgd_R1"])
   output:
      out=basedir_data+"/ReadPrep/classreads_qualtrim_adptr_{trimmed}.tsv"
   script:
      "scripts/DataExtraction/dataConcat.py"

# Combine all read preparation files
rule read_preparation_details_sample:
   input:
      raw_R1 = lambda w: paired_reads[w["sample"]]["R1"],
      raw_R2 = lambda w: paired_reads[w["sample"]]["R1"],
      trimmed_R1 = expand(fastqlocations["trimmed"],readtype="R1", allow_missing=True),
      trimmed_R2 = expand(fastqlocations["trimmed"],readtype="R2", allow_missing=True),
      inserts = rules.merge_paired_reads.output.inserts,




# Combine bbmap and sina datasets
rule combine_bbmap_sina_file:
   input:
      bam = rules.filter_ssu_reads.output.bam,
      sina = rules.classify_ssu.output.csv
   output:
      out=basedir_data+"/classification_data/{sample}_readclassification_SSU.tsv",
   conda:
      "envs/R.yml"
   script:
      "scripts/DataExtraction/CombineBBMapAndSinaData.R"

# Do the SINA BBMap combination for all samples
rule readclass_SSU:
   input:
      files = expand(rules.combine_bbmap_sina_file.output,sample=sample_names),
   params:
      colnames=["sample"],
      vals=sample_names
   output:
      out=basedir_data+"/classification_data/readclassification_SSU.tsv"
   script:
      "scripts/DataExtraction/dataConcat.py"

# Add header to kaiju file 
rule kaiju_data_sample:
   input:
      rules.kaiju_sample.output.out
   output:
      basedir_data+"/classification_data/{sample}_readclassification_Kaiju.tsv"
   shell:
      "echo 'Classified\tReadname\ttaxid\tlengthorscore\ttaxid_matches\taccession_matches\tmatching_fragments\ttax' > {output} && cat {input} >> {output}"

# Combine kaiju details for all samples
rule kaiju_data:
   input:
      files = expand(rules.kaiju_data_sample.output,sample=sample_names),
   params:
      colnames=["sample"],
      vals=sample_names
   output:
      out=basedir_data+"/classification_data/readclassification_Kaiju.tsv"
   script:
      "scripts/DataExtraction/dataConcat.py"

# Annotate custom BLAST results
rule customblastanno:
   input:
      blastxres=basedir_readanno+"/{sample}/{sample}_prot_{db}_blast.tsv",
      datafile=basedir_dbs+"/readanno/prot/{db}.tsv",
   params:
      blastxcolnames=blastcolumns,
   output:
      basedir_data+"/read_annotation/{sample}_readanno_{db}.tsv"
   conda:
      "envs/R.yml"
   script:
      "scripts/DataExtraction/mergeDBintoBLASTXres.R"

# Combine the result files for the samples
rule annodata:
   input:
      files = expand(rules.customblastanno.output,sample=sample_names,db="{db}"),
   params:
      colnames=["sample"],
      vals=sample_names
   output:
      out=basedir_data+"/read_annotation/readanno_{db}.tsv"
   script:
      "scripts/DataExtraction/dataConcat.py"

# Metaquast for all samples
rule metaquastdata:
   input:
      files = expand(rules.metaquast_sample.output.tsv,sample=sample_names),
   params:
      colnames=["sample"],
      vals=sample_names
   output:
      out=basedir_data+"/assembly/metaquast.tsv"
   script:
      "scripts/DataExtraction/dataConcat.py"

# Convert coverage statistics to data frame
rule covdata_sample:
   input:
      rules.cal_map_cov.output.cov_sum
   output:
      basedir_data+"/assembly/{sample}_cov_pileup_summary.tsv"
   conda:
      "envs/R.yml"
   script:
      "scripts/DataExtraction/bbcoverageParser.R"

# Pileup.sh statistics
rule covdata:
   input:
      files = expand(rules.covdata_sample.output,sample=sample_names),
   params:
      colnames=["sample"],
      vals=sample_names
   output:
      out=basedir_data+"/assembly/cov_pileup_summary.tsv"
   script:
      "scripts/DataExtraction/dataConcat.py"

# Pileup.sh details
rule covdata_details:
   input:
      files = expand(rules.cal_map_cov.output.cov,sample=sample_names),
   params:
      colnames=["sample"],
      vals=sample_names
   output:
      out=basedir_data+"/assembly/cov_pileup_details.tsv"
   script:
      "scripts/DataExtraction/dataConcat.py"

# Remove the BAM file specific columns from the jgi_depth files
rule jgi_depthdata_sample:
   input:
      rules.metabat_depth.output
   output:
      basedir_data+"/assembly/{sample}_cov_jgi_details.tsv"
   shell:
      "cut -f 1,2,3 {input} > {output}"

# Combine the reduce jgi_depth files
rule jgi_depthdata:
   input:
      files = expand(rules.jgi_depthdata_sample.output,sample=sample_names),
   params:
      colnames=["sample"],
      vals=sample_names
   output:
      out=basedir_data+"/assembly/cov_pileup_details.tsv"
   script:
      "scripts/DataExtraction/dataConcat.py"

# Combine all checkm lineage wf results
rule checkm_lineage:
   input:
      files = expand(rules.checkm_sample.output.sreport,sample=sample_names),
   params:
      colnames=["sample"],
      vals=sample_names,
      header="bin_id\tmarker_lineage\tlineage_genomes\tlineage_markers\tlineage_marker_sets\t0_sets\t1_sets\t2_sets\t3_sets\t4_sets\t5_or_more_sets\tcompleteness\tcontaimination\tstrain_heterogeneity\n"
   output:
      out=basedir_data+"/binning/checkm_lineage.tsv"
   script:
      "scripts/DataExtraction/dataConcat.py"

# Combine all checkm profile results
rule checkmprofile:
   input:
      files = expand(rules.checkm_extra_data.output.profile,sample=sample_names),
   params:
      colnames=["sample"],
      vals=sample_names,
      header="bin_id\tbin_size_Mbp\tmapped_reads\tmapped_reads_perc\tof_bin_population_perc\tpopulation_perc\n"
   output:
      out=basedir_data+"/binning/checkm_profile.tsv"
   script:
      "scripts/DataExtraction/dataConcat.py"

# Combine any checkm help file for all samples
rule checkmextrafiles:
   input:
      files = expand(basedir_binning+"/checkm/{sample}/{sample}_{{file}}.tsv",sample=sample_names),
   params:
      colnames=["sample"],
      vals=sample_names
   output:
      out = basedir_data+"/binning/checkm_all_{file}.tsv",
   script:
      "scripts/DataExtraction/dataConcat.py"
